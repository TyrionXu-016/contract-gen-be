import sys
sys.stdout.reconfigure(encoding='utf-8')

from langchain.text_splitter import CharacterTextSplitter  # ç”¨äºæ–‡æœ¬åˆ†å—çš„å·¥å…·åº“
import spacy  # ç”¨äºä¸­æ–‡åˆ†è¯å’Œæ–‡æœ¬è§£æçš„æ ¸å¿ƒåº“

from flk_crawler import crawl_laws

# åŠ è½½ä¸­æ–‡åˆ†è¯æ¨¡å‹
nlp = spacy.load("zh_core_web_sm")

# ====================== 1. çˆ¬è™«è¾“å…¥æ¥å£ï¼šæ¥æ”¶ä¸Šæ¸¸æ¨¡å—æ•°æ® ======================
def receive_crawl_data(crawl_data: dict) -> tuple[str, str, str]:
    """
    é€‚é…çˆ¬è™«æ¨¡å—çš„è¾“å‡ºæ ¼å¼ï¼Œæå–æ ¸å¿ƒä¿¡æ¯
    :param crawl_data: çˆ¬è™«è¿”å›çš„å•æ¡æ•°æ®å­—å…¸ï¼ˆæ¥è‡ªcrawl_lawsçš„ç»“æœï¼‰
    :return: data_id, data_type, raw_text
    """
    # ä»çˆ¬è™«ç»“æœä¸­æå–å­—æ®µï¼Œå¯¹åº”çˆ¬è™«çš„è¿”å›æ ¼å¼
    data_id = crawl_data.get("id", "default_id")
    # çˆ¬è™«æŠ“å–çš„æ˜¯æ³•è§„ï¼Œæ‰€ä»¥æ•°æ®ç±»å‹å›ºå®šä¸º"law"
    data_type = "law"
    # è¯»å–txtæ–‡ä»¶å†…å®¹ä½œä¸ºåŸå§‹æ–‡æœ¬ï¼ˆçˆ¬è™«å·²è‡ªåŠ¨ç”Ÿæˆtxtï¼‰
    raw_text = ""
    txt_path = crawl_data.get("txt_path", "")
    if txt_path:
        try:
            with open(txt_path, "r", encoding="utf-8") as f:
                raw_text = f.read()
        except Exception as e:
            print(f"è¯»å–txtæ–‡ä»¶å¤±è´¥ï¼š{e}")
    return data_id, data_type, raw_text

# ====================== 2. åˆ†å—æ ¸å¿ƒé€»è¾‘ ======================
def split_contract(raw_text: str, data_type: str) -> list[str]:
    if data_type == "law":
        splitter = CharacterTextSplitter(separator="ç¬¬", chunk_size=200, chunk_overlap=0)
        blocks = splitter.split_text(raw_text)
        blocks = ["ç¬¬" + b for b in blocks if b]
    elif data_type == "case":
        blocks = [p for p in raw_text.split("\n") if p.strip()]
    else:
        doc = nlp(raw_text)
        blocks = []
        current_block = ""
        for token in doc:
            if token.text in ["ä¸€", "äºŒ", "ä¸‰", "1.", "2.", "ï¼ˆ", "ï¼‰"] and current_block:
                blocks.append(current_block.strip())
                current_block = token.text
            else:
                current_block += token.text
        if current_block:
            blocks.append(current_block.strip())
    return blocks

# ====================== 3. å‘é‡åº“è¾“å‡ºæ¥å£ ======================
def send_to_vector_db(data_id: str, data_type: str, blocks: list[str]) -> list[dict]:
    structured_blocks = []
    for idx, block_content in enumerate(blocks):
        structured_blocks.append({
            "data_id": data_id,
            "block_id": f"{data_id}_block_{idx+1}",
            "block_type": data_type,
            "block_content": block_content
        })
    return structured_blocks

# ====================== 4. ä¸»å‡½æ•°ï¼šä¸²è”çˆ¬è™«+åˆ†å—+å‘é‡åº“æµç¨‹ ======================
if __name__ == "__main__":
    # ========== æ­¥éª¤1ï¼šè°ƒç”¨çˆ¬è™«æ¥å£ï¼ŒæŠ“å–çœŸå®æ³•è§„æ•°æ® ==========
    print("ğŸ“Œ å¼€å§‹æŠ“å–æ³•è§„æ•°æ®...")
    # é…ç½®çˆ¬è™«å‚æ•°ï¼šå…³é”®è¯ã€ç¿»é¡µæ•°ç­‰
    crawl_results = crawl_laws(
        keyword="æ°‘æ³•å…¸",  # å¯æ›¿æ¢ä¸º"å…¬å¸æ³•""åˆåŒæ³•"ç­‰
        max_pages=2,       # æŠ“å–2é¡µç»“æœï¼Œå¯è°ƒæ•´
        auto_txt=True      # è‡ªåŠ¨ç”Ÿæˆtxtæ–‡ä»¶ï¼Œå¿…é¡»å¼€å¯
    )
    print(f"âœ… çˆ¬è™«å®Œæˆï¼Œå…±æŠ“å– {len(crawl_results)} æ¡æ³•è§„æ•°æ®\n")

    # ========== æ­¥éª¤2ï¼šå¾ªç¯å¤„ç†æ¯æ¡çˆ¬è™«æ•°æ® ==========
    for idx, crawl_data in enumerate(crawl_results, start=1):
        print(f"===== å¤„ç†ç¬¬ {idx} æ¡æ•°æ®ï¼š{crawl_data.get('title')} =====")
        
        # è°ƒç”¨è¾“å…¥æ¥å£ï¼Œæå–ä¿¡æ¯
        data_id, data_type, raw_text = receive_crawl_data(crawl_data)
        if not raw_text:
            print("âŒ è¯¥æ¡æ•°æ®æ— txtå†…å®¹ï¼Œè·³è¿‡\n")
            continue
        print(f"ğŸ“„ æå–æ–‡æœ¬é•¿åº¦ï¼š{len(raw_text)} å­—")

        # è°ƒç”¨åˆ†å—é€»è¾‘
        split_blocks = split_contract(raw_text, data_type)
        print(f"ğŸ”§ åˆ†å—å®Œæˆï¼Œå…± {len(split_blocks)} ä¸ªåˆ†å—")

        # è°ƒç”¨è¾“å‡ºæ¥å£ï¼Œç”Ÿæˆå‘é‡åº“æ•°æ®
        vector_data = send_to_vector_db(data_id, data_type, split_blocks)
        print(f"ğŸ“Š ç”Ÿæˆå‘é‡åº“æ•°æ® {len(vector_data)} æ¡\n")